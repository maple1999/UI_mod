# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'Main_window.ui'
#
# Created by: PyQt5 UI code generator 5.15.11
#
# WARNING: Any manual changes made to this file will be lost when pyuic5 is
# run again.  Do not edit this file unless you know what you are doing.

import argparse
import os
import subprocess
import sys

import numpy as np
import pandas as pd
import torch
import yaml
# from PyQt5.QtWidgets import QMessageBox, QFileDialog, QLineEdit
from PyQt5.QtGui import *
from PyQt5.QtWidgets import *
from PyQt5.QtCore import *
from PyQt5.uic.properties import QtGui
from PyQt5 import QtCore, QtGui, QtWidgets

from untitled import Ui_MainWindow
import cv2
from matplotlib import pyplot as plt
from mediapipe.python.solutions import drawing_utils as mp_drawing
from mediapipe.python.solutions import pose as mp_pose
from inference_and_visualization_camera import main
import threading
from model import PoseRAC, Action_trigger
from screeninfo import get_monitors
import mediapipe as mp
import subprocess
from inference_and_visualization_camera import PoseClassificationVisualizer, get_landmarks, normalize_landmarks


class MainWindow(QMainWindow, Ui_MainWindow):
    def __init__(self, parent=None):
        super(MainWindow, self).__init__(parent)

        self.setupUi(self)
        self.CAM_NUM = 0
        self.cap = cv2.VideoCapture()
        # 计时器用于刷新摄像头画面
        self.timer = QTimer(self)
        self.timer.timeout.connect(self.update_camera_frame)
        self.camera_active = False  # 摄像头状态
        self.background()
        self.video_thread = None # 初始化线程
        self.action_index_mapping = {"front_raise": 0, "pull_up": 1, "squat": 2, "bench_pressing": 3,
                                     "jump_jack": 4, "situp": 5, "push_up": 6, "pommelhorse": 7}
        self.motion = "front_raise" #初始化，后续需要先设置默认值
        self.real_index = None
        self.localtion = None #初始化，后续需要先设置默认值
        #self.init_timer()

    def background(self):
        self.pushButton.clicked.connect(self.toggle_camera)  # 控制相机
        self.action.triggered.connect(self.display_human_pose)
        self.pushButton_2.clicked.connect(self.motion_detection)  # 控制姿势检测
    def update_motion(self, motion_text):
        self.motion = motion_text
        self.real_index = self.action_index_mapping[self.motion]
        print(f"当前 motion 更新为: {self.motion}")
        print(f"当前 real_index 更新为: {self.real_index}")


    # def handle_selection(self, menu, motion , localtion): #处理菜单栏选项
    #     print("cehis")
    #     if menu == "menu_3":
    #         self.motion = motion
    #         print(f"当前 motion 更新为:{self.motion}")
    #     elif menu == "menu_4":
    #         self.localtion = localtion
    #         print(f"当前 localtion 更新为:{self.localtion}")
    def toggle_camera(self):  # 控制摄像头状态
        if self.camera_active:
            self.stop_camera()
        else:
            self.start_camera()

    def start_camera(self):
        self.cap = cv2.VideoCapture(0)  # 打开默认摄像头
        if not self.cap.isOpened():
            QMessageBox.warning(self, "警告", "无法打开摄像头")
            return

        self.timer.start(30)
        self.pushButton.setText("关闭摄像头")
        self.camera_active = True

    def stop_camera(self):
        self.timer.stop()
        if self.cap is not None:
            self.cap.release()
            self.cap = None
        self.label_2.clear()  # 清除画面
        self.pushButton.setText("开启摄像头")
        self.camera_active = False

    def update_camera_frame(self):
        ret, frame = self.cap.read()
        if ret:
            # 转换图像格式
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            height, width, channel = frame.shape
            bytes_per_line = 3 * width
            q_image = QImage(frame.data, width, height, bytes_per_line, QImage.Format_RGB888)
            q_image_scaled = q_image.scaled(self.label_2.size(), QtCore.Qt.KeepAspectRatio)
            pixmap = QPixmap.fromImage(q_image_scaled)

            # 设置摄像头标签图像
            self.label_2.setPixmap(pixmap)
    def display_human_pose(self):
        if not self.cap.isOpened():
            QMessageBox.warning(self, "警告", "无法打开摄像头")
            return

        # 获取 label3 的尺寸
        label_size = self.label_3.size()

        # 创建并启动视频线程
        self.video_thread = VideoThread(self.cap, label_size)
        self.video_thread.update_frame.connect(self.update_label3)
        self.video_thread.start()
    def update_label3(self, q_image):
        # 将处理后的图像显示在 label3 中
        q_image_scaled = q_image.scaled(self.label_3.size(), QtCore.Qt.KeepAspectRatio)
        pixmap = QPixmap.fromImage(q_image_scaled)
        self.label_3.setPixmap(pixmap)

    def closeEvent(self, event):
        if self.video_thread is not None:
            self.video_thread.stop()
        self.cap.release()
        cv2.destroyAllWindows()
        event.accept()




    def motion_detection(self):
        csv_label_path = '..\\all_action_realtime.csv'
        root_dir = '..\\RepCount_pose'
        print("进入motion_detection函数")
        label_pd = pd.read_csv(csv_label_path)
        index2action = {}
        length_label = len(label_pd.index)
        for label_i in range(length_label):
            one_data = label_pd.iloc[label_i]
            action = one_data['action']
            label = one_data['label']
            index2action[label] = action
        num_classes = len(index2action)

        model = PoseRAC(None, None, None, None, dim=99, heads=9,
                        enc_layer=6, learning_rate=0.001,
                        seed=42, num_classes=num_classes, alpha=0.012)
        weight_path = '..\\best_weights_PoseRAC.pth'
        new_weights = torch.load(weight_path, map_location='cpu')
        model.load_state_dict(new_weights)
        model.eval()
        print("调用PoseRAC模型")
        enter_threshold = 0.78
        exit_threshold = 0.4
        momentum = 0.4
        real_index = self.real_index
        print(real_index)
        print(f"当前选择的 motion 更新为: {self.motion}")
        action_type = index2action[real_index] #检测动作类型，这部分还未实现
#
#
#       此部分功能需完善
#
#
        repetition_salient_1 = Action_trigger(
            action_name=action_type,
            enter_threshold=enter_threshold,
            exit_threshold=exit_threshold
        )
        repetition_salient_2 = Action_trigger(
            action_name=action_type,
            enter_threshold=enter_threshold,
            exit_threshold=exit_threshold
        )
        classify_prob = 0.5
        pose_count = 0
        curr_pose = 'holder'
        init_pose = 'pose_holder'

        #使用相机
        if not self.cap.isOpened():
            QMessageBox.information(self, "警告", "请检查设备", QMessageBox.Ok)
            return
        video_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        video_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

        pose_tracker = mp_pose.Pose()
        pose_classification_visualizer = PoseClassificationVisualizer(
            class_name=action_type,
            plot_x_max=500,
            plot_y_max=1
        )
        frame_idx = 0
        frame_count = 0
        tensors = None

        while True:
            success, input_frame = self.cap.read()
            if not success:
                break
            frame_count += 1

            input_frame = cv2.cvtColor(input_frame, cv2.COLOR_BGR2RGB)
            result = pose_tracker.process(image=input_frame)
            pose_landmarks = result.pose_landmarks

            output_frame = input_frame.copy()
            if pose_landmarks is not None:
                mp_drawing.draw_landmarks(image=output_frame, landmark_list=pose_landmarks,
                                          connections=mp_pose.POSE_CONNECTIONS)
            else:
                continue

            landmarks = get_landmarks(pose_landmarks)
            normalized_landmarks = normalize_landmarks(landmarks)
            normalized_landmarks = normalized_landmarks[0].reshape(-1, 99)
            tensor = torch.from_numpy(normalized_landmarks).float()

            if tensors is None:
                tensors = tensor
            else:
                if tensors.shape[0] == 100:
                    tensors = tensor[1:]
                tensors = torch.cat((tensors, tensor), 0)

            if tensors.shape[0] < 30:
                continue

            output = torch.sigmoid(model(tensors))[-1]
            output_numpy = output[real_index].detach().cpu().numpy()
            classify_prob = output_numpy * (1 - momentum) + momentum *classify_prob

            salient1_triggered = repetition_salient_1(classify_prob)
            reverse_classify_prob = 1 - classify_prob
            salient2_triggered = repetition_salient_2(reverse_classify_prob)

            if init_pose == 'pose_holder':
                if salient1_triggered:
                    init_pose = 'salient1'
                elif salient2_triggered:
                    init_pose = 'sa;ient2'

            if init_pose == 'salinet1':
                if curr_pose == 'salient1' and salient2_triggered:
                    pose_count += 1
            else:
                if curr_pose == 'salient2' and salient1_triggered:
                    pose_count += 1

            if salient1_triggered:
                curr_pose = 'salient1'
            elif salient2_triggered:
                curr_pose = 'salient2'

            output_frame = pose_classification_visualizer(
                frame=output_frame,
                pose_classification=classify_prob,
                pose_classification_filtered=classify_prob,
                repetitions_count=pose_count
            )

            output_frame = cv2.cvtColor(np.array(output_frame), cv2.COLOR_RGB2BGR) #颜色转换
            save_picture = output_frame.copy() #创建图像副本，方便绘制文本信息
            frame_idx += 1 #帧计数
            # 使用 cv2.putText() 在图像上绘制文本。
            # show_text = 'action: {}'.format(action_type) 创建要显示的文本内容，这里是动作类型。
            # 文本位置由 org = (int(video_width * 0.1), int(video_height * 0.9)) 决定，文本显示在图像的左下方。
            # 文本的字体、大小、颜色、厚度等属性由 font, fontScale, color, thickness 等参数设置。
            # 此部分需要更改，由控件控制是否显示
            font = cv2.FONT_HERSHEY_SIMPLEX
            org = (int(video_width * 0.1), int(video_height * 0.9))
            fontScale = 1
            color = (0, 0, 255)
            thickness = 3
            show_text = 'action:{}'.format(action_type)
            save_picture = cv2.putText(save_picture, show_text, org, font,
                                       fontScale, color, thickness, cv2.LINE_AA)
            save_picture = cv2.cvtColor(save_picture, cv2.COLOR_BGR2RGB)
            # 将 numpy.ndarray 转换为 QImage
            height, width, channel = save_picture.shape
            bytes_per_line = 3 * width
            q_image = QtGui.QImage(save_picture.data, width, height, bytes_per_line, QtGui.QImage.Format_RGB888)

            # 将 QImage 转换为 QPixmap
            q_pixmap = QtGui.QPixmap.fromImage(q_image)
            q_pixmap_scaled = q_pixmap.scaled(self.label_3.size(), QtCore.Qt.KeepAspectRatio)
            # 使用 setPixmap 显示图像
            self.label_3.setPixmap(q_pixmap_scaled)

            if cv2.waitKey(5) & 0xFF == ord('q'):
                break


        #pose_tracker.close()
        self.cap.release()
        cv2.destroyAllWindows()



    def stop(self):
        self.running = False
        self.quit()
        self.wait()

class VideoThread(QThread):  #视频处理线程
    update_frame = pyqtSignal(QImage)

    def __init__(self, cap, label_size):
        super().__init__()
        self.cap = cap
        self.label_size = label_size
        self.pose_tracker = mp_pose.Pose()
        self.running = True

    def run(self):
        while self.running and self.cap.isOpened():
            ret, frame = self.cap.read()
            if not ret:
                break

            # 转换为 RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # 处理帧并获取姿势数据
            results = self.pose_tracker.process(frame_rgb)

            # 如果检测到姿势，绘制关键点和骨架
            if results.pose_landmarks:
                mp_drawing.draw_landmarks(
                    frame_rgb, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)

            # 转换为 QImage 并发出信号更新 UI
            height, width, channel = frame_rgb.shape
            bytes_per_line = 3 * width
            q_image = QImage(frame_rgb.data, width, height, bytes_per_line, QImage.Format_RGB888)
            q_image_scaled = q_image.scaled(self.label_size, QtCore.Qt.KeepAspectRatio)
            self.update_frame.emit(q_image_scaled)

            # 添加一个小的延迟来控制帧率
            cv2.waitKey(1)




if __name__ == "__main__":
    app = QApplication(sys.argv)
    player = MainWindow()
    player.show()
    sys.exit(app.exec_())




